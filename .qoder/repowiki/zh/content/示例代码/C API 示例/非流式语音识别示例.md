# 非流式语音识别示例

<cite>
**本文档引用的文件**
- [decode-file-c-api.c](file://c-api-examples/decode-file-c-api.c)
- [dolphin-ctc-c-api.c](file://c-api-examples/dolphin-ctc-c-api.c)
- [fire-red-asr-c-api.c](file://c-api-examples/fire-red-asr-c-api.c)
- [moonshine-c-api.c](file://c-api-examples/moonshine-c-api.c)
- [nemo-canary-c-api.c](file://c-api-examples/nemo-canary-c-api.c)
- [paraformer-c-api.c](file://c-api-examples/paraformer-c-api.c)
- [CMakeLists.txt](file://c-api-examples/CMakeLists.txt)
- [c-api.h](file://sherpa-onnx/c-api/c-api.h)
</cite>

## 目录
1. [简介](#简介)
2. [非流式ASR模型特点与适用场景](#非流式asr模型特点与适用场景)
3. [示例代码解析](#示例代码解析)
4. [编译与运行指南](#编译与运行指南)
5. [性能与精度对比及选型建议](#性能与精度对比及选型建议)
6. [非流式识别内部处理流程](#非流式识别内部处理流程)
7. [结论](#结论)

## 简介

sherpa-onnx 是一个支持多种语音处理功能的开源项目，包括语音识别（ASR）、语音合成（TTS）、说话人分离、说话人识别等。本项目支持在本地运行这些功能，适用于多种平台和操作系统，如Linux、macOS、Windows、Android、iOS和HarmonyOS。此外，sherpa-onnx 还支持多种编程语言，包括C++、C、Python、JavaScript、Java、C#、Kotlin、Swift、Go、Dart、Rust和Pascal，并且支持WebAssembly。

本项目中的非流式语音识别（Non-Streaming ASR）是指在处理音频文件时，一次性读取整个音频文件并进行识别，而不是逐段处理。这种方式适用于需要高精度识别的场景，尤其是在处理长音频文件时。非流式ASR模型通常具有更高的准确率，但处理时间较长。

**Section sources**
- [README.md](file://README.md)

## 非流式ASR模型特点与适用场景

### Dolphin CTC 模型

Dolphin CTC 模型是一种基于连接时序分类（Connectionist Temporal Classification, CTC）的非流式ASR模型。它适用于多语言环境，能够处理多种语言的语音识别任务。Dolphin CTC 模型的特点是模型轻量，适合在资源受限的设备上运行。其适用场景包括：

- 多语言语音识别
- 资源受限设备上的语音识别
- 需要快速响应的实时应用

### Fire Red ASR 模型

Fire Red ASR 模型是一种基于Transformer架构的非流式ASR模型。它具有较高的准确率和鲁棒性，适用于复杂环境下的语音识别任务。Fire Red ASR 模型的特点是模型较大，需要较多的计算资源。其适用场景包括：

- 高精度语音识别
- 复杂环境下的语音识别
- 需要高鲁棒性的应用

### Moonshine 模型

Moonshine 模型是一种基于深度神经网络的非流式ASR模型。它结合了多种先进的技术，如注意力机制和残差连接，以提高识别准确率。Moonshine 模型的特点是模型结构复杂，但识别效果优秀。其适用场景包括：

- 高精度语音识别
- 长音频文件的处理
- 需要高准确率的应用

### Nemo Canary 模型

Nemo Canary 模型是一种基于NVIDIA NeMo框架的非流式ASR模型。它支持多种语言和方言，具有良好的泛化能力。Nemo Canary 模型的特点是模型训练数据丰富，能够处理多种语言的语音识别任务。其适用场景包括：

- 多语言语音识别
- 方言识别
- 需要良好泛化能力的应用

### Paraformer 模型

Paraformer 模型是一种基于Transformer架构的非流式ASR模型。它结合了并行解码技术，能够在保证高准确率的同时提高处理速度。Paraformer 模型的特点是模型结构简洁，处理速度快。其适用场景包括：

- 高精度语音识别
- 快速处理长音频文件
- 需要高效处理的应用

**Section sources**
- [dolphin-ctc-c-api.c](file://c-api-examples/dolphin-ctc-c-api.c)
- [fire-red-asr-c-api.c](file://c-api-examples/fire-red-asr-c-api.c)
- [moonshine-c-api.c](file://c-api-examples/moonshine-c-api.c)
- [nemo-canary-c-api.c](file://c-api-examples/nemo-canary-c-api.c)
- [paraformer-c-api.c](file://c-api-examples/paraformer-c-api.c)

## 示例代码解析

### decode-file-c-api.c

`decode-file-c-api.c` 是一个示例程序，展示了如何使用 sherpa-onnx 的C API来解码音频文件。该程序的主要步骤如下：

1. **初始化配置**：设置模型配置参数，包括模型文件路径、线程数、提供者（provider）等。
2. **创建识别器**：使用配置参数创建一个离线识别器。
3. **创建流**：为识别器创建一个离线流。
4. **读取音频文件**：读取指定的音频文件。
5. **接受波形数据**：将音频文件的波形数据传递给流。
6. **解码流**：调用解码函数对流进行解码。
7. **获取结果**：从流中获取识别结果。
8. **清理资源**：释放识别器和流的资源。

```c
// 初始化配置
SherpaOnnxOfflineModelConfig offline_model_config;
memset(&offline_model_config, 0, sizeof(offline_model_config));
offline_model_config.debug = 1;
offline_model_config.num_threads = 1;
offline_model_config.provider = "cpu";
offline_model_config.tokens = tokens_filename;
offline_model_config.dolphin.model = model_filename;

// 创建识别器
const SherpaOnnxOfflineRecognizer *recognizer =
    SherpaOnnxCreateOfflineRecognizer(&recognizer_config);

// 创建流
const SherpaOnnxOfflineStream *stream =
    SherpaOnnxCreateOfflineStream(recognizer);

// 读取音频文件
const SherpaOnnxWave *wave = SherpaOnnxReadWave(wav_filename);

// 接受波形数据
SherpaOnnxAcceptWaveformOffline(stream, wave->sample_rate, wave->samples,
                                wave->num_samples);

// 解码流
SherpaOnnxDecodeOfflineStream(recognizer, stream);

// 获取结果
const SherpaOnnxOfflineRecognizerResult *result =
    SherpaOnnxGetOfflineStreamResult(stream);

// 清理资源
SherpaOnnxDestroyOfflineRecognizerResult(result);
SherpaOnnxDestroyOfflineStream(stream);
SherpaOnnxDestroyOfflineRecognizer(recognizer);
SherpaOnnxFreeWave(wave);
```

**Section sources**
- [decode-file-c-api.c](file://c-api-examples/decode-file-c-api.c)

## 编译与运行指南

### CMakeLists.txt 配置

`CMakeLists.txt` 文件用于配置项目的构建过程。以下是 `c-api-examples` 目录下的 `CMakeLists.txt` 文件的部分内容：

```cmake
include(cargs)

include_directories(${PROJECT_SOURCE_DIR})
add_executable(decode-file-c-api decode-file-c-api.c)
target_link_libraries(decode-file-c-api sherpa-onnx-c-api cargs)

add_executable(kws-c-api kws-c-api.c)
target_link_libraries(kws-c-api sherpa-onnx-c-api)

# 其他可执行文件的配置
```

### 依赖管理

为了编译和运行这些示例程序，需要安装以下依赖：

- **ONNX Runtime**：用于运行ONNX模型。
- **CMake**：用于构建项目。
- **GCC** 或 **Clang**：用于编译C代码。

### 编译步骤

1. **克隆仓库**：
   ```sh
   git clone https://github.com/k2-fsa/sherpa-onnx.git
   cd sherpa-onnx
   ```

2. **安装依赖**：
   ```sh
   sudo apt-get update
   sudo apt-get install cmake g++ libonnxruntime-dev
   ```

3. **构建项目**：
   ```sh
   mkdir build
   cd build
   cmake ..
   make
   ```

4. **运行示例**：
   ```sh
   ./bin/decode-file-c-api --tokens=/path/to/tokens.txt \
                           --model=/path/to/model.onnx \
                           /path/to/audio.wav
   ```

**Section sources**
- [CMakeLists.txt](file://c-api-examples/CMakeLists.txt)

## 性能与精度对比及选型建议

### 性能对比

| 模型 | 处理速度 | 内存占用 | 计算资源需求 |
|------|---------|---------|-------------|
| Dolphin CTC | 快 | 低 | 低 |
| Fire Red ASR | 慢 | 高 | 高 |
| Moonshine | 中 | 中 | 中 |
| Nemo Canary | 慢 | 高 | 高 |
| Paraformer | 快 | 中 | 中 |

### 精度对比

| 模型 | 准确率 | 鲁棒性 | 泛化能力 |
|------|-------|-------|---------|
| Dolphin CTC | 中 | 中 | 中 |
| Fire Red ASR | 高 | 高 | 高 |
| Moonshine | 高 | 高 | 高 |
| Nemo Canary | 高 | 高 | 高 |
| Paraformer | 高 | 高 | 高 |

### 选型建议

- **资源受限设备**：推荐使用 Dolphin CTC 模型，因为它轻量且处理速度快。
- **高精度需求**：推荐使用 Fire Red ASR、Moonshine 或 Nemo Canary 模型，因为它们具有较高的准确率和鲁棒性。
- **快速处理长音频文件**：推荐使用 Paraformer 模型，因为它结合了并行解码技术，处理速度快且准确率高。

**Section sources**
- [dolphin-ctc-c-api.c](file://c-api-examples/dolphin-ctc-c-api.c)
- [fire-red-asr-c-api.c](file://c-api-examples/fire-red-asr-c-api.c)
- [moonshine-c-api.c](file://c-api-examples/moonshine-c-api.c)
- [nemo-canary-c-api.c](file://c-api-examples/nemo-canary-c-api.c)
- [paraformer-c-api.c](file://c-api-examples/paraformer-c-api.c)

## 非流式识别内部处理流程

### 音频解码

非流式ASR模型首先需要将音频文件解码为原始的波形数据。这一步骤通常使用音频解码库（如FFmpeg）完成。解码后的波形数据是一个一维数组，表示音频信号的幅度值。

### 特征提取

解码后的波形数据需要转换为适合模型输入的特征向量。常用的特征提取方法包括梅尔频谱图（Mel-spectrogram）和梅尔频率倒谱系数（MFCC）。这些特征向量能够捕捉音频信号的频域信息，有助于模型更好地理解语音内容。

### 模型推理

特征向量被送入ASR模型进行推理。模型通过多层神经网络处理特征向量，生成对应的文本序列。推理过程包括前向传播和解码两个阶段：

1. **前向传播**：输入特征向量通过模型的各个层，生成中间表示。
2. **解码**：中间表示被解码为最终的文本序列。常用的解码方法包括贪婪搜索（greedy search）和束搜索（beam search）。

### 结果输出

解码后的文本序列被输出为最终的识别结果。结果通常以JSON格式返回，包含识别的文本、时间戳等信息。

**Section sources**
- [c-api.h](file://sherpa-onnx/c-api/c-api.h)

## 结论

sherpa-onnx 提供了一套完整的非流式语音识别解决方案，支持多种模型和编程语言。通过合理选择模型和优化配置，可以在不同应用场景下实现高效、准确的语音识别。希望本文档能帮助开发者更好地理解和使用 sherpa-onnx 的非流式ASR功能。

**Section sources**
- [README.md](file://README.md)