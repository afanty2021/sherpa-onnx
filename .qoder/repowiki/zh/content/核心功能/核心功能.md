# 核心功能

<cite>
**本文档中引用的文件**  
- [voice-activity-detector.cc](file://sherpa-onnx/csrc/voice-activity-detector.cc)
- [voice-activity-detector.h](file://sherpa-onnx/csrc/voice-activity-detector.h)
- [offline-tts.h](file://sherpa-onnx/csrc/offline-tts.h)
- [offline-stream.cc](file://sherpa-onnx/csrc/offline-stream.cc)
- [speaker-embedding-manager.h](file://sherpa-onnx/csrc/speaker-embedding-manager.h)
- [speaker-embedding-manager.cc](file://sherpa-onnx/csrc/speaker-embedding-manager.cc)
- [offline-speaker-diarization-pyannote-impl.h](file://sherpa-onnx/csrc/offline-speaker-diarization-pyannote-impl.h)
- [keyword-spotter.h](file://sherpa-onnx/csrc/keyword-spotter.h)
- [offline-speech-denoiser-gtcrn-impl.h](file://sherpa-onnx/csrc/offline-speech-denoiser-gtcrn-impl.h)
- [vad-model-config.h](file://sherpa-onnx/csrc/vad-model-config.h)
- [online-stream.h](file://sherpa-onnx/csrc/online-stream.h)
- [offline-lm-config.h](file://sherpa-onnx/csrc/offline-lm-config.h)
- [parse-options.cc](file://sherpa-onnx/csrc/parse-options.cc)
- [cxx-api.cc](file://sherpa-onnx/c-api/cxx-api.cc)
- [offline_recognizer.dart](file://flutter/sherpa_onnx/lib/src/offline_recognizer.dart)
- [sherpa-onnx-offline-speaker-diarization.cc](file://sherpa-onnx/csrc/sherpa-onnx-offline-speaker-diarization.cc)
- [non-streaming-tts.js](file://scripts/node-addon-api/lib/non-streaming-tts.js)
- [spoken-language-identification.js](file://scripts/node-addon-api/lib/spoken-language-identification.js)
- [speaker-identification.js](file://scripts/node-addon-api/lib/speaker-identification.js)
- [vad.js](file://scripts/node-addon-api/lib/vad.js)
- [offline-speaker-diarization.py](file://python-api-examples/offline-speaker-diarization.py)
- [offline-speech-enhancement-gtcrn.py](file://python-api-examples/offline-speech-enhancement-gtcrn.py)
- [test_keyword_spotter_transducer.js](file://nodejs-addon-examples/test_keyword_spotter_transducer.js)
- [test_audio_tagging_ced.js](file://nodejs-addon-examples/test_audio_tagging_ced.js)
- [test_asr_streaming_ctc_hlg.js](file://nodejs-addon-examples/test_asr_streaming_ctc_hlg.js)
- [streaming-zipformer-rtf-cxx-api.cc](file://cxx-api-examples/streaming-zipformer-rtf-cxx-api.cc)
- [timer.h](file://sherpa-onnx/csrc/timer.h)
</cite>

## 目录
1. [引言](#引言)
2. [语音活动检测（VAD）](#语音活动检测vad)
3. [语音识别（ASR）](#语音识别asr)
4. [语音合成（TTS）](#语音合成tts)
5. [说话人识别与分离](#说话人识别与分离)
6. [关键词检测](#关键词检测)
7. [音频标签](#音频标签)
8. [语音增强与源分离](#语音增强与源分离)
9. [语种识别](#语种识别)
10. [功能集成示例](#功能集成示例)
11. [性能基准测试](#性能基准测试)
12. [配置选项与调优建议](#配置选项与调优建议)

## 引言
sherpa-onnx是一个功能丰富的语音处理框架，提供了包括语音识别（ASR）、语音合成（TTS）、说话人分离、说话人识别、语音活动检测（VAD）、关键词检测、音频标签、语音增强和源分离在内的多种核心功能。该框架基于ONNX运行时，支持跨平台部署，并提供了C++、Python、Node.js、Java、Go等多种语言的API接口。本文档将深入探讨这些核心功能的技术原理、算法选择、性能特征以及配置选项，为开发者提供全面的使用指南。

## 语音活动检测（VAD）
语音活动检测（Voice Activity Detection, VAD）是语音处理中的关键预处理步骤，用于检测音频流中的语音段。sherpa-onnx实现了基于Silero VAD和Ten VAD模型的语音活动检测功能。`VoiceActivityDetector`类是VAD功能的核心实现，它通过`AcceptWaveform`方法接收音频波形数据，并通过`Compute`方法计算语音活动概率。当检测到语音开始时，系统会创建一个新的`SpeechSegment`对象来记录语音段的起始位置和样本数据。该功能支持配置采样率、线程数、执行提供程序（如CPU或GPU）等参数，以适应不同的应用场景和性能需求。

**Section sources**
- [voice-activity-detector.cc](file://sherpa-onnx/csrc/voice-activity-detector.cc#L1-L231)
- [voice-activity-detector.h](file://sherpa-onnx/csrc/voice-activity-detector.h#L1-L63)
- [vad-model-config.h](file://sherpa-onnx/csrc/vad-model-config.h#L1-L47)

## 语音识别（ASR）
语音识别（Automatic Speech Recognition, ASR）是将语音信号转换为文本的过程。sherpa-onnx支持多种ASR模型，包括流式和非流式模型。`OnlineStream`类是流式ASR的核心，它负责处理实时音频流，提取特征，并与解码器协同工作。该类提供了`AcceptWaveform`方法来接收音频数据，`NumFramesReady`方法来查询已准备好的特征帧数，以及`GetFrames`方法来获取特征数据。框架支持多种解码器，如CTC解码器、Transducer解码器和Paraformer解码器，以适应不同的模型架构和性能要求。此外，还支持上下文图（Context Graph）功能，用于热词增强，提高特定词汇的识别准确率。

**Section sources**
- [online-stream.h](file://sherpa-onnx/csrc/online-stream.h#L1-L122)
- [offline-lm-config.h](file://sherpa-onnx/csrc/offline-lm-config.h#L1-L47)
- [parse-options.cc](file://sherpa-onnx/csrc/parse-options.cc#L414-L452)
- [cxx-api.cc](file://sherpa-onnx/c-api/cxx-api.cc#L228-L257)
- [offline_recognizer.dart](file://flutter/sherpa_onnx/lib/src/offline_recognizer.dart#L471-L498)

## 语音合成（TTS）
语音合成（Text-to-Speech, TTS）是将文本转换为自然语音的过程。sherpa-onnx提供了`OfflineTts`类来实现非流式语音合成功能。该类的`Generate`方法接受文本、说话人ID（sid）和语速（speed）作为输入参数，并返回包含合成音频样本和采样率的`GeneratedAudio`对象。对于多说话人模型，可以通过设置`sid`参数来选择不同的说话人。`speed`参数用于控制合成语音的速度，例如2.0表示两倍速。该功能支持回调机制，当处理完一定数量的句子后会调用指定的回调函数，便于实现进度更新或分段处理。

**Section sources**
- [offline-tts.h](file://sherpa-onnx/csrc/offline-tts.h#L78-L96)
- [non-streaming-tts.js](file://scripts/node-addon-api/lib/non-streaming-tts.js#L1-L25)
- [NonStreamingTts.ets](file://harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/ets/components/NonStreamingTts.ets#L95-L112)
- [non-streaming-tts.cc](file://harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-tts.cc#L625-L661)

## 说话人识别与分离
说话人识别与分离功能包括说话人识别（Speaker Identification）和说话人分离（Speaker Diarization）。`SpeakerEmbeddingExtractor`类用于提取说话人嵌入向量，而`SpeakerEmbeddingManager`类则用于管理和搜索说话人。`SpeakerEmbeddingManager`的`Add`方法可以将说话人名称和对应的嵌入向量列表添加到数据库中，`Search`方法则通过计算余弦相似度来识别说话人。说话人分离功能通过`OfflineSpeakerDiarization`类实现，它结合了语音活动检测和说话人嵌入提取，将音频中的不同说话人区分开来。该功能首先使用分割模型确定语音段，然后为每个（语音段，说话人）对计算嵌入向量，最后通过聚类算法将相同说话人的片段归为一类。

**Section sources**
- [speaker-embedding-manager.h](file://sherpa-onnx/csrc/speaker-embedding-manager.h#L35-L68)
- [speaker-embedding-manager.cc](file://sherpa-onnx/csrc/speaker-embedding-manager.cc#L212-L257)
- [offline-speaker-diarization-pyannote-impl.h](file://sherpa-onnx/csrc/offline-speaker-diarization-pyannote-impl.h#L442-L476)
- [speaker-identification.js](file://scripts/node-addon-api/lib/speaker-identification.js#L1-L53)
- [offline-speaker-diarization.py](file://python-api-examples/offline-speaker-diarization.py#L107-L136)
- [sherpa-onnx-offline-speaker-diarization.cc](file://sherpa-onnx/csrc/sherpa-onnx-offline-speaker-diarization.cc#L109-L138)

## 关键词检测
关键词检测（Keyword Spotting, KWS）功能用于在音频流中检测预定义的关键词。`KeywordSpotter`类是该功能的核心，其配置参数包括`keywords_score`和`keywords_threshold`，分别用于控制关键词的得分和检测阈值。`isReady`方法用于检查解码器是否准备好进行解码，`decode`方法执行实际的解码过程，而`getResult`方法则返回检测结果。当检测到关键词后，需要调用`reset`方法重置流，以便开始下一轮检测。该功能支持从文件或麦克风输入音频，并可以与VAD功能结合使用，以提高检测效率和准确性。

**Section sources**
- [keyword-spotter.h](file://sherpa-onnx/csrc/keyword-spotter.h#L53-L102)
- [test_keyword_spotter_transducer.js](file://nodejs-addon-examples/test_keyword_spotter_transducer.js#L40-L64)
- [test-keyword-spotter-transducer.js](file://nodejs-examples/test-keyword-spotter-transducer.js#L35-L52)

## 音频标签
音频标签（Audio Tagging）功能用于识别音频中的声音事件或场景。`OfflineStream`类支持CED（Collaborative Ensemble for Detection）和Zipformer等模型，用于音频标签任务。配置参数包括帧长、预加重系数、窗函数类型等，这些参数会影响特征提取的质量。`AcceptWaveform`方法用于接收音频数据，而`Compute`方法则计算每个声音事件的概率。该功能可以识别多种声音事件，如音乐、语音、噪声等，并为每个事件提供概率评分。结果通常以事件名称和对应概率的形式输出，便于后续处理和分析。

**Section sources**
- [offline-stream.cc](file://sherpa-onnx/csrc/offline-stream.cc#L122-L162)
- [test_audio_tagging_ced.js](file://nodejs-addon-examples/test_audio_tagging_ced.js#L52-L63)

## 语音增强与源分离
语音增强与源分离功能旨在提高语音质量并分离混合音频中的不同声源。`OfflineSpeechDenoiser`类实现了基于GTCRN（Gated Temporal Convolutional Recurrent Network）模型的语音去噪功能。该类通过`Process`方法处理STFT（短时傅里叶变换）结果，利用循环神经网络状态来增强语音信号。`istft`（逆短时傅里叶变换）模块负责将增强后的频域信号转换回时域，生成去噪后的音频。该功能支持配置模型文件、线程数和执行提供程序，以平衡性能和质量。源分离功能可以将混合音频中的语音、音乐和噪声等不同成分分离出来，适用于会议记录、电话录音等场景。

**Section sources**
- [offline-speech-denoiser-gtcrn-impl.h](file://sherpa-onnx/csrc/offline-speech-denoiser-gtcrn-impl.h#L76-L110)
- [offline-speech-enhancement-gtcrn.py](file://python-api-examples/offline-speech-enhancement-gtcrn.py#L1-L86)

## 语种识别
语种识别（Spoken Language Identification）功能用于识别语音中的语言种类。`SpokenLanguageIdentification`类通过创建非流式ASR流来处理音频，并返回语言代码（如en代表英语，zh代表中文）。该功能基于多语言ASR模型，能够识别多种语言。`compute`方法执行实际的语种识别过程，返回最可能的语言代码。该功能可以与VAD功能结合使用，先检测语音段，再对每个语音段进行语种识别，从而实现对长音频的多语言内容分析。

**Section sources**
- [spoken-language-identification.js](file://scripts/node-addon-api/lib/spoken-language-identification.js#L1-L30)

## 功能集成示例
sherpa-onnx的核心优势之一是其功能的可集成性。一个典型的集成示例是将VAD与ASR结合使用。首先，使用`VoiceActivityDetector`检测音频中的语音段，然后将每个语音段传递给`OfflineRecognizer`进行语音识别。这种方法可以显著提高ASR的效率，因为它避免了对静音段的处理。另一个示例是将说话人识别与VAD结合，先检测语音段，再提取说话人嵌入向量，最后与已知说话人数据库进行比对。这种集成方式适用于安全认证、个性化服务等场景。此外，还可以将关键词检测与语音增强结合，先对音频进行去噪处理，再进行关键词检测，以提高在嘈杂环境下的检测准确率。

**Section sources**
- [vad.js](file://scripts/node-addon-api/lib/vad.js#L66-L96)
- [speaker-identification-with-vad.py](file://python-api-examples/speaker-identification-with-vad.py)

## 性能基准测试
性能基准测试是评估语音处理系统效率的重要手段。sherpa-onnx在多个示例中提供了性能测试代码，通过计算实时因子（Real Time Factor, RTF）来评估性能。RTF是处理音频所花费的时间与音频时长的比值，RTF小于1表示系统能够实时处理。例如，在ASR示例中，通过记录处理开始和结束时间，计算`elapsed_seconds`和`audio_duration`，然后计算`real_time_factor = elapsed_seconds / duration`。测试结果通常包括波形时长、处理耗时和RTF值。这些基准测试结果可以帮助开发者了解系统在不同硬件配置和模型下的性能表现，为优化提供依据。

**Section sources**
- [test_asr_streaming_ctc_hlg.js](file://nodejs-addon-examples/test_asr_streaming_ctc_hlg.js#L47-L56)
- [streaming-zipformer-rtf-cxx-api.cc](file://cxx-api-examples/streaming-zipformer-rtf-cxx-api.cc#L121-L132)
- [timer.h](file://sherpa-onnx/csrc/timer.h#L1-L29)

## 配置选项与调优建议
sherpa-onnx提供了丰富的配置选项，允许开发者根据具体需求进行调优。主要配置参数包括：
- **num_threads**: 控制并行处理的线程数，增加线程数可以提高处理速度，但也会增加CPU负载。
- **provider**: 指定ONNX运行时的执行提供程序，如CPU、CUDA或CoreML，选择合适的提供程序可以显著提升性能。
- **sample_rate**: 音频采样率，通常为16000Hz，确保输入音频与此匹配以避免重采样开销。
- **debug**: 启用调试模式，输出详细的日志信息，有助于问题排查。

调优建议：
1. 在资源充足的服务器上，使用CUDA提供程序和多线程以获得最佳性能。
2. 在嵌入式设备上，优先考虑CPU提供程序和单线程以降低功耗。
3. 对于实时应用，选择RTF较低的模型和配置。
4. 定期更新模型文件以利用最新的算法改进。

**Section sources**
- [vad-model-config.h](file://sherpa-onnx/csrc/vad-model-config.h#L1-L47)
- [parse-options.cc](file://sherpa-onnx/csrc/parse-options.cc#L414-L452)
- [cxx-api.cc](file://sherpa-onnx/c-api/cxx-api.cc#L228-L257)