[æ ¹ç›®å½•](../CLAUDE.md) > **python-api-examples**

# Python API ç¤ºä¾‹

> æ›´æ–°æ—¶é—´ï¼š2025-12-10 07:44:45

## æ¨¡å—èŒè´£

æœ¬æ¨¡å—åŒ…å« Sherpa-ONNX çš„ Python API ä½¿ç”¨ç¤ºä¾‹ï¼Œå±•ç¤ºäº†å¦‚ä½•é€šè¿‡ Python è°ƒç”¨å„ç§è¯­éŸ³å¤„ç†åŠŸèƒ½ã€‚è¿™äº›ç¤ºä¾‹æ¶µç›–äº†ï¼š

1. **è¯­éŸ³è¯†åˆ«**ï¼šåœ¨çº¿/ç¦»çº¿ã€æµå¼/éæµå¼è¯†åˆ«
2. **è¯­éŸ³åˆæˆ**ï¼šå¤šç§ TTS æ¨¡å‹ä½¿ç”¨
3. **è¯´è¯äººå¤„ç†**ï¼šè¯†åˆ«ã€æ—¥å¿—ã€éªŒè¯
4. **å®æ—¶å¤„ç†**ï¼šéº¦å…‹é£è¾“å…¥ã€WebSocket æœåŠ¡
5. **é«˜çº§åº”ç”¨**ï¼šå­—å¹•ç”Ÿæˆã€æœåŠ¡å™¨éƒ¨ç½²

## ç¤ºä¾‹åˆ†ç±»

### ğŸ¤ è¯­éŸ³è¯†åˆ« (ASR)

#### åœ¨çº¿è¯†åˆ«ï¼ˆæµå¼ï¼‰
- `online-decode-files.py` - åŸºç¡€åœ¨çº¿æ–‡ä»¶è¯†åˆ«
- `streaming-paraformer-asr-microphone.py` - Paraformer éº¦å…‹é£å®æ—¶è¯†åˆ«
- `streaming-zipformer-ctc-hlg-decode-file.py` - Zipformer CTC + HLG è§£ç 
- `two-pass-speech-recognition-from-microphone.py` - ä¸¤éè¯†åˆ«ï¼ˆå¿«é€Ÿ+ç²¾è°ƒï¼‰

#### ç¦»çº¿è¯†åˆ«ï¼ˆéæµå¼ï¼‰
- `offline-decode-files.py` - åŸºç¡€ç¦»çº¿æ–‡ä»¶è¯†åˆ«
- `offline-whisper-decode-files.py` - Whisper æ¨¡å‹ä½¿ç”¨
- `offline-sense-voice-ctc-decode-files.py` - SenseVoice å¤šè¯­è¨€è¯†åˆ«
- `offline-zipformer-ctc-decode-files.py` - Zipformer CTC æ¨¡å‹
- `offline-moonshine-decode-files.py` - Moonshine è½»é‡æ¨¡å‹

#### ç‰¹æ®Šæ¨¡å‹
- `offline-nemo-parakeet-decode-file.py` - NVIDIA NeMo Transducer
- `offline-dolphin-ctc-decode-files.py` - Dolphin å¤šè¯­è¨€æ¨¡å‹
- `offline-fire-red-asr-decode-files.py` - Fire Red ASR
- `offline-telespeech-ctc-decode-files.py` - ç§‘å¤§è®¯é£ TeleSpeech
- `offline-omnilingual-asr-ctc-decode-files.py` - 1600ç§è¯­è¨€æ¨¡å‹

### ğŸ”Š è¯­éŸ³åˆæˆ (TTS)

#### åŸºç¡€ TTS
- `offline-tts.py` - åŸºç¡€ç¦»çº¿ TTS
- `offline-tts-play.py` - TTS å¹¶æ’­æ”¾
- `offline-zeroshot-tts.py` - é›¶æ ·æœ¬å£°éŸ³å…‹éš†

#### ç‰¹å®š TTS æ¨¡å‹
```python
# VITS æ¨¡å‹
vits_model = {
    "vits": "./vits-model.onnx",
    "lexicon": "./lexicon.txt",
}

# Matcha TTS
matcha_model = {
    "matcha": "./matcha-model.onnx",
}

# Kokoro å¤šè¯­è¨€
kokoro_model = {
    "kokoro": "./kokoro-model.onnx",
    "lexicon": "./kokoro-lexicon.txt",
}
```

### ğŸ‘¥ è¯´è¯äººå¤„ç†

#### è¯´è¯äººè¯†åˆ«
- `speaker-identification.py` - è¯´è¯äººè¯†åˆ«åŸºç¡€
- `speaker-identification-with-vad.py` - ç»“åˆ VAD çš„è¯†åˆ«
- `speaker-identification-with-vad-dynamic.py` - åŠ¨æ€è¯´è¯äººè¯†åˆ«

#### è¯´è¯äººæ—¥å¿—
- `offline-speaker-diarization.py` - ç¦»çº¿è¯´è¯äººæ—¥å¿—
- `offline-speaker-diarization.py` ç¤ºä¾‹ä»£ç ï¼š
```python
import sherpa_onnx

# é…ç½®è¯´è¯äººæ—¥å¿—
config = sherpa_onnx.OfflineSpeakerDiarizationConfig(
    vad_config=vad_config,
    embedding_config=embedding_config,
    clustering_config=clustering_config,
)

# åˆ›å»ºå¤„ç†å®ä¾‹
sd = sherpa_onnx.OfflineSpeakerDiarization(config)

# å¤„ç†éŸ³é¢‘
audio = sherpa_onnx.read_wav("audio.wav")
segments = sd.process(audio)

# è¾“å‡ºç»“æœ
for seg in segments.segments:
    print(f"æ—¶é—´: {seg.start:.2f} - {seg.end:.2f}")
    print(f"è¯´è¯äºº: {seg.speaker}")
```

### ğŸŒ æœåŠ¡ç«¯åº”ç”¨

#### WebSocket æœåŠ¡
- `streaming_server.py` - æµå¼è¯†åˆ«æœåŠ¡å™¨
- `non_streaming_server.py` - éæµå¼è¯†åˆ«æœåŠ¡å™¨
- `two-pass-wss.py` - ä¸¤éè¯†åˆ« WebSocket æœåŠ¡
- `http_server.py` - HTTP REST API æœåŠ¡

#### WebSocket å®¢æˆ·ç«¯
- `online-websocket-client-decode-file.py` - æ–‡ä»¶è§£ç å®¢æˆ·ç«¯
- `online-websocket-client-microphone.py` - éº¦å…‹é£å®¢æˆ·ç«¯
- `offline-websocket-client-decode-files-sequential.py` - é¡ºåºè§£ç 

### ğŸ¯ é«˜çº§åŠŸèƒ½

#### VADï¼ˆè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼‰
- `vad-microphone.py` - éº¦å…‹é£ VAD
- `vad-alsa.py` - Linux ALSA VAD
- `vad-with-non-streaming-asr.py` - VAD + ASR ç»„åˆ

#### è¯­éŸ³å¢å¼º
- `offline-speech-enhancement-gtcrn.py` - GT-CRN è¯­éŸ³å¢å¼º
- `offline-source-separation-spleeter.py` - Spleeter éŸ³æºåˆ†ç¦»
- `offline-source-separation-uvr.py` - UVR éŸ³æºåˆ†ç¦»

#### å®ç”¨å·¥å…·
- `generate-subtitles.py` - å­—å¹•ç”Ÿæˆ
- `add-punctuation.py` - æ ‡ç‚¹æ¢å¤
- `spoken-language-identification.py` - è¯­è¨€è¯†åˆ«

## ä½¿ç”¨ç¤ºä¾‹

### 1. å¿«é€Ÿå¼€å§‹ - è¯­éŸ³è¯†åˆ«

```python
import sherpa_onnx
import wave

# 1. åˆ›å»ºè¯†åˆ«å™¨
recognizer_config = sherpa_onnx.OfflineRecognizerConfig(
    feat_config=sherpa_onnx.FeatureConfig(
        sampling_rate=16000,
        feature_dim=80,
    ),
    model_config=sherpa_onnx.OfflineModelConfig(
        transducer=sherpa_onnx.OfflineTransducerModelConfig(
            model="./model.onnx",
        ),
        tokens="./tokens.txt",
        num_threads=2,
    ),
)

recognizer = sherpa_onnx.OfflineRecognizer(recognizer_config)

# 2. è¯»å–éŸ³é¢‘
with wave.open("test.wav", "rb") as wf:
    audio = wf.readframes(-1)
    audio = np.frombuffer(audio, dtype=np.float32)

# 3. è¯†åˆ«
stream = recognizer.create_stream()
stream.accept_waveform(16000, audio)
recognizer.decode_stream(stream)

# 4. è·å–ç»“æœ
result = stream.result
print(f"è¯†åˆ«ç»“æœ: {result.text}")
```

### 2. å®æ—¶è¯­éŸ³è¯†åˆ«

```python
import sherpa_onnx
import pyaudio

# é…ç½®
 recognizer_config = sherpa_onnx.OnlineRecognizerConfig(...)
 recognizer = sherpa_onnx.OnlineRecognizer(recognizer_config)

# åˆ›å»ºæµ
stream = recognizer.create_stream()

# éŸ³é¢‘å›è°ƒ
def callback(in_data, frame_count, time_info, status):
    samples = np.frombuffer(in_data, dtype=np.float32)
    stream.accept_waveform(16000, samples)

    # è·å–éƒ¨åˆ†ç»“æœ
    if recognizer.is_ready(stream):
        recognizer.decode_stream(stream)
        result = recognizer.get_result(stream)
        if result.text:
            print(f"éƒ¨åˆ†ç»“æœ: {result.text}")

    return (in_data, pyaudio.paContinue)

# å¯åŠ¨éŸ³é¢‘æµ
p = pyaudio.PyAudio()
stream_audio = p.open(
    format=pyaudio.paFloat32,
    channels=1,
    rate=16000,
    input=True,
    frames_per_buffer=1600,
    stream_callback=callback,
)

stream_audio.start_stream()
stream_audio.wait_for_completion()
```

### 3. TTS è¯­éŸ³åˆæˆ

```python
import sherpa_onnx
import soundfile as sf

# é…ç½® TTS
tts_config = sherpa_onnx.OfflineTtsConfig(
    model=sherpa_onnx.OfflineTtsModelConfig(
        vits=sherpa_onnx.OfflineTtsVitsModelConfig(
            model="./vits.onnx",
            lexicon="./lexicon.txt",
        ),
        tokens="./tokens.txt",
        num_threads=2,
    ),
    rule_fsts="./rule.fst",
)

# åˆ›å»º TTS å®ä¾‹
tts = sherpa_onnx.OfflineTts(tts_config)

# ç”Ÿæˆè¯­éŸ³
audio = tts.generate("ä½ å¥½ï¼Œä¸–ç•Œï¼")

# ä¿å­˜
sf.write("output.wav", audio.samples, audio.sample_rate)

# æ’­æ”¾ï¼ˆå¯é€‰ï¼‰
tts.play(audio)
```

## æœ€ä½³å®è·µ

### 1. æ€§èƒ½ä¼˜åŒ–
```python
# ä½¿ç”¨æ¨¡å‹ç¼“å­˜
recognizer = sherpa_onnx.OfflineRecognizer(config)
# å¤ç”¨è¯†åˆ«å™¨å¤„ç†å¤šä¸ªæ–‡ä»¶

# æ‰¹å¤„ç†
for audio_batch in audio_batches:
    results = recognizer.recognize_batch(audio_batch)
```

### 2. é”™è¯¯å¤„ç†
```python
try:
    recognizer = sherpa_onnx.OfflineRecognizer(config)
except RuntimeError as e:
    print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    sys.exit(1)
```

### 3. èµ„æºç®¡ç†
```python
# ä½¿ç”¨ with è¯­å¥ï¼ˆå¦‚æœæ”¯æŒï¼‰
with sherpa_onnx.OfflineRecognizer(config) as recognizer:
    result = recognizer.recognize(audio)

# æˆ–æ‰‹åŠ¨é‡Šæ”¾
recognizer = sherpa_onnx.OfflineRecognizer(config)
try:
    # ä½¿ç”¨ recognizer
    pass
finally:
    del recognizer
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ¨¡å‹åŠ è½½å¤±è´¥**
   - æ£€æŸ¥æ¨¡å‹è·¯å¾„
   - éªŒè¯æ¨¡å‹æ ¼å¼
   - ç¡®è®¤ ONNX Runtime ç‰ˆæœ¬

2. **éŸ³é¢‘æ ¼å¼é”™è¯¯**
   - ç¡®ä¿é‡‡æ ·ç‡åŒ¹é…
   - æ£€æŸ¥éŸ³é¢‘æ•°æ®ç±»å‹ï¼ˆfloat32ï¼‰
   - éªŒè¯é€šé“æ•°ï¼ˆå•å£°é“ï¼‰

3. **æ€§èƒ½é—®é¢˜**
   - å‡å°‘çº¿ç¨‹æ•°
   - ä½¿ç”¨é‡åŒ–æ¨¡å‹
   - å¯ç”¨ GPU åŠ é€Ÿ

### è°ƒè¯•æŠ€å·§

```python
# å¯ç”¨è°ƒè¯•æ¨¡å¼
config.model_config.debug = True

# æ‰“å°é…ç½®
print(recognizer_config)

# æ£€æŸ¥æ¨¡å‹ä¿¡æ¯
print(f"æ¨¡å‹è·¯å¾„: {config.model_config.transducer.model}")
```

## å˜æ›´è®°å½•

### 2025-12-10 07:44:45
- ğŸ“ åˆ›å»º Python ç¤ºä¾‹æ–‡æ¡£
- ğŸ·ï¸ åˆ†ç±»æ•´ç†ç¤ºä¾‹ä»£ç 
- ğŸ’» æ·»åŠ ä½¿ç”¨ç¤ºä¾‹å’Œæœ€ä½³å®è·µ
- ğŸ”§ è®°å½•æ•…éšœæ’é™¤æŒ‡å—

---

*ç›¸å…³æ–‡ä»¶ï¼š[Python ç»‘å®š](../sherpa-onnx/python/CLAUDE.md) | [C++ æ ¸å¿ƒ](../sherpa-onnx/csrc/CLAUDE.md) | [C API ç¤ºä¾‹](../c-api-examples/CLAUDE.md)*