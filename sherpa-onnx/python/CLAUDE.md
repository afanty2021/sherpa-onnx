[æ ¹ç›®å½•](../../CLAUDE.md) > [sherpa-onnx](../) > **python**

# Python ç»‘å®š

> æ›´æ–°æ—¶é—´ï¼š2025-12-10 07:48:28

## æ¨¡å—èŒè´£

Sherpa-ONNX çš„ Python ç»‘å®šæ¨¡å—æä¾›äº†å°† C++ æ ¸å¿ƒåŠŸèƒ½æš´éœ²ç»™ Python è¯­è¨€çš„æ¥å£ã€‚é€šè¿‡ pybind11 æ¡†æ¶ï¼Œå®ƒå®ç°äº†é«˜æ•ˆçš„æ— ç¼é›†æˆï¼Œè®© Python å¼€å‘è€…èƒ½å¤Ÿè½»æ¾ä½¿ç”¨æ‰€æœ‰è¯­éŸ³ AI åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š

- **è¯­éŸ³è¯†åˆ« (ASR)**ï¼šåœ¨çº¿/ç¦»çº¿è¯†åˆ«
- **è¯­éŸ³åˆæˆ (TTS)**ï¼šå¤šæ¨¡å‹æ”¯æŒ
- **è¯´è¯äººå¤„ç†**ï¼šè¯†åˆ«ã€æ—¥å¿—ã€éªŒè¯
- **è¯­éŸ³å¢å¼º**ï¼šé™å™ªã€éŸ³æºåˆ†ç¦»
- **å…¶ä»–åŠŸèƒ½**ï¼šVADã€å…³é”®è¯æ£€æµ‹ã€è¯­è¨€è¯†åˆ«

## å…¥å£ä¸å¯åŠ¨

### ä¸»è¦ç»‘å®šæ–‡ä»¶
```
sherpa-onnx/python/csrc/
â”œâ”€â”€ offline-recognizer.cc     # ç¦»çº¿è¯†åˆ«ç»‘å®š
â”œâ”€â”€ online-recognizer.cc      # åœ¨çº¿è¯†åˆ«ç»‘å®š
â”œâ”€â”€ offline-tts.cc            # TTS ç»‘å®š
â”œâ”€â”€ speaker-*.cc              # è¯´è¯äººç›¸å…³ç»‘å®š
â”œâ”€â”€ vad.cc                    # VAD ç»‘å®š
â””â”€â”€ keyword-spotter.cc        # å…³é”®è¯æ£€æµ‹ç»‘å®š
```

### æ„å»ºå’Œå®‰è£…
```bash
# æ–¹æ³•1ï¼šä»æºç æ„å»º
cd sherpa-onnx
mkdir build && cd build
cmake -DCMAKE_BUILD_TYPE=Release \
      -DSHERPA_ONNX_ENABLE_PYTHON=ON \
      -DPython3_EXECUTABLE=$(which python3) \
      ..
make -j4

# å®‰è£…åˆ° Python ç¯å¢ƒ
cd python
pip install -e .

# æ–¹æ³•2ï¼šä½¿ç”¨ pip å®‰è£…é¢„ç¼–è¯‘åŒ…
pip install sherpa-onnx
```

## å¯¹å¤–æ¥å£

### 1. è¯­éŸ³è¯†åˆ«æ¥å£

#### ç¦»çº¿è¯­éŸ³è¯†åˆ«
```python
import sherpa_onnx

# é…ç½®ç¦»çº¿è¯†åˆ«å™¨
config = sherpa_onnx.OfflineRecognizerConfig(
    feat_config=sherpa_onnx.FeatureExtractorConfig(
        sampling_rate=16000,
        feature_dim=80,
    ),
    model_config=sherpa_onnx.OfflineModelConfig(
        transducer=sherpa_onnx.OfflineTransducerModelConfig(
            encoder="./encoder.onnx",
            decoder="./decoder.onnx",
            joiner="./joiner.onnx",
        ),
        tokens="./tokens.txt",
        num_threads=2,
        debug=False,
        provider="cpu",
    ),
    decoding_method="greedy_search",
    max_active_paths=4,
)

# åˆ›å»ºè¯†åˆ«å™¨
recognizer = sherpa_onnx.OfflineRecognizer(config)

# åˆ›å»ºéŸ³é¢‘æµ
stream = recognizer.create_stream()
stream.accept_waveform(16000, audio_samples)

# æ‰§è¡Œè¯†åˆ«
recognizer.decode_stream(stream)
result = stream.result

print(f"è¯†åˆ«ç»“æœ: {result.text}")
```

#### åœ¨çº¿è¯­éŸ³è¯†åˆ«
```python
# é…ç½®åœ¨çº¿è¯†åˆ«å™¨
online_config = sherpa_onnx.OnlineRecognizerConfig(
    feat_config=sherpa_onnx.FeatureExtractorConfig(
        sampling_rate=16000,
        feature_dim=80,
    ),
    model_config=sherpa_onnx.OnlineModelConfig(
        zipformer=sherpa_onnx.OnlineZipformerModelConfig(
            encoder="./encoder.onnx",
            decoder="./decoder.onnx",
            joiner="./joiner.onnx",
        ),
        tokens="./tokens.txt",
        num_threads=2,
    ),
    endpoint_config=sherpa_onnx.EndpointConfig(),
    lm_config=sherpa_onnx.OnlineLMConfig(),
)

# åˆ›å»ºè¯†åˆ«å™¨
online_recognizer = sherpa_onnx.OnlineRecognizer(online_config)

# åˆ›å»ºæµ
stream = online_recognizer.create_stream()

# å®æ—¶å¤„ç†éŸ³é¢‘
def process_audio_chunk(audio_chunk):
    stream.accept_waveform(16000, audio_chunk)

    # è·å–éƒ¨åˆ†ç»“æœ
    if online_recognizer.is_ready(stream):
        online_recognizer.decode_stream(stream)
        result = online_recognizer.get_result(stream)
        if result.text:
            print(f"éƒ¨åˆ†ç»“æœ: {result.text}")
```

### 2. è¯­éŸ³åˆæˆæ¥å£

```python
# é…ç½® TTS
tts_config = sherpa_onnx.OfflineTtsConfig(
    model=sherpa_onnx.OfflineTtsModelConfig(
        vits=sherpa_onnx.OfflineTtsVitsModelConfig(
            model="./vits.onnx",
            lexicon="./lexicon.txt",
            tokens="./tokens.txt",
        ),
        num_threads=2,
    ),
    rule_fsts="./rule.fst",
    max_num_sentences=1,
)

# åˆ›å»º TTS å®ä¾‹
tts = sherpa_onnx.OfflineTts(tts_config)

# ç”Ÿæˆè¯­éŸ³
audio = tts.generate(
    text="ä½ å¥½ï¼Œä¸–ç•Œï¼",
    speaker_id=0,
    speed=1.0,
)

# audio åŒ…å«:
# - audio.samples: éŸ³é¢‘æ•°æ® (numpy array)
# - audio.sample_rate: é‡‡æ ·ç‡
```

### 3. è¯´è¯äººè¯†åˆ«æ¥å£

```python
# é…ç½®è¯´è¯äººè¯†åˆ«
speaker_config = sherpa_onnx.SpeakerEmbeddingExtractorConfig(
    model="./speaker_model.onnx",
    num_threads=2,
)

# åˆ›å»ºæå–å™¨
extractor = sherpa_onnx.SpeakerEmbeddingExtractor(speaker_config)

# æå–è¯´è¯äººç‰¹å¾
stream = extractor.create_stream()
stream.accept_waveform(16000, audio_samples)
embedding = extractor.compute(stream)

# è¯´è¯äººéªŒè¯
verifier_config = sherpa_onnx.SpeakerEmbeddingVerifierConfig(
    extractor_config=speaker_config,
    threshold=0.5,
)
verifier = sherpa_onnx.SpeakerEmbeddingVerifier(verifier_config)

# éªŒè¯è¯´è¯äºº
score = verifier.verify(embedding1, embedding2)
print(f"ç›¸ä¼¼åº¦åˆ†æ•°: {score}")
```

### 4. VAD æ¥å£

```python
# é…ç½® VAD
vad_config = sherpa_onnx.VadModelConfig(
    silero_vad=sherpa_onnx.SileroVadModelConfig(
        model="./silero_vad.onnx",
        threshold=0.5,
        min_silence_duration=0.5,
        min_speech_duration=0.25,
    ),
    sample_rate=16000,
    window_size=512,
)

# åˆ›å»º VAD å®ä¾‹
vad = sherpa_onnx.VoiceActivityDetector(vad_config)

# å¤„ç†éŸ³é¢‘
vad.accept_waveform(audio_chunk)
if vad.is_speech_detected():
    print("æ£€æµ‹åˆ°è¯­éŸ³")
```

## å…³é”®ä¾èµ–ä¸é…ç½®

### æ ¸å¿ƒä¾èµ–
- **pybind11**: C++/Python ç»‘å®šæ¡†æ¶
- **ONNX Runtime**: æ¨¡å‹æ¨ç†å¼•æ“
- **NumPy**: éŸ³é¢‘æ•°æ®å¤„ç†
- **librosa** (å¯é€‰): éŸ³é¢‘åˆ†æ

### é…ç½®ç±»å±‚æ¬¡ç»“æ„

```
é…ç½®ç±»
â”œâ”€â”€ FeatureExtractorConfig      # ç‰¹å¾æå–é…ç½®
â”œâ”€â”€ OfflineModelConfig          # ç¦»çº¿æ¨¡å‹é…ç½®
â”‚   â”œâ”€â”€ OfflineTransducerModelConfig
â”‚   â”œâ”€â”€ OfflineParaformerModelConfig
â”‚   â”œâ”€â”€ OfflineWhisperModelConfig
â”‚   â””â”€â”€ ...
â”œâ”€â”€ OnlineModelConfig           # åœ¨çº¿æ¨¡å‹é…ç½®
â”‚   â”œâ”€â”€ OnlineZipformerModelConfig
â”‚   â”œâ”€â”€ OnlineParaformerModelConfig
â”‚   â””â”€â”€ ...
â”œâ”€â”€ OfflineTtsModelConfig       # TTS æ¨¡å‹é…ç½®
â”‚   â”œâ”€â”€ OfflineTtsVitsModelConfig
â”‚   â”œâ”€â”€ OfflineTtsMatchaModelConfig
â”‚   â””â”€â”€ ...
â”œâ”€â”€ SpeakerEmbeddingExtractorConfig  # è¯´è¯äººæ¨¡å‹é…ç½®
â””â”€â”€ VadModelConfig              # VAD æ¨¡å‹é…ç½®
```

## æ•°æ®æ¨¡å‹

### ä¸»è¦æ•°æ®ç»“æ„

#### è¯†åˆ«ç»“æœ
```python
class OfflineRecognitionResult:
    text: str                 # è¯†åˆ«æ–‡æœ¬
    tokens: List[str]         # Token åºåˆ—
    timestamps: List[float]   # æ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼‰
```

#### TTS éŸ³é¢‘
```python
class GeneratedAudio:
    samples: np.ndarray       # éŸ³é¢‘æ•°æ® (float32)
    sample_rate: int          # é‡‡æ ·ç‡
```

#### è¯´è¯äººåˆ†å‰²ç»“æœ
```python
class SpeakerDiarizationResult:
    segments: List[Segment]   # è¯­éŸ³æ®µ

class Segment:
    start: float              # å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
    end: float                # ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
    speaker: int              # è¯´è¯äºº ID
```

## æµ‹è¯•ä¸è´¨é‡

### å•å…ƒæµ‹è¯•
```python
# è¿è¡Œæµ‹è¯•
python -m pytest tests/

# æˆ–ä½¿ç”¨ unittest
python -m unittest discover tests/
```

### æ€§èƒ½æµ‹è¯•
```python
import time

# æµ‹è¯•è¯†åˆ«é€Ÿåº¦
start_time = time.time()
result = recognizer.recognize(audio)
duration = time.time() - start_time

rtf = duration / (len(audio) / sample_rate)
print(f"å®æ—¶å› å­ (RTF): {rtf:.2f}")
```

## å¸¸è§é—®é¢˜ (FAQ)

### Q1: å¦‚ä½•å¤„ç†ä¸åŒçš„éŸ³é¢‘æ ¼å¼ï¼Ÿ
A: ä½¿ç”¨ librosa æˆ– soundfile è½¬æ¢ï¼š
```python
import soundfile as sf

# è¯»å–éŸ³é¢‘
audio, sr = sf.read("input.wav")
if sr != 16000:
    # é‡é‡‡æ ·
    import librosa
    audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)
```

### Q2: å¦‚ä½•å‡å°‘å†…å­˜ä½¿ç”¨ï¼Ÿ
A:
1. ä½¿ç”¨é‡åŒ–æ¨¡å‹
2. å‡å°‘çº¿ç¨‹æ•°
3. åŠæ—¶é‡Šæ”¾éŸ³é¢‘æ•°æ®
```python
# ä½¿ç”¨å®Œåæ¸…ç†
del stream
del audio_chunk
```

### Q3: å¦‚ä½•æé«˜å¤„ç†é€Ÿåº¦ï¼Ÿ
A:
1. ä½¿ç”¨ GPU åŠ é€Ÿï¼š
```python
config.model_config.provider = "cuda"
```
2. å¢åŠ çº¿ç¨‹æ•°ï¼ˆåœ¨åˆç†èŒƒå›´å†…ï¼‰
3. ä½¿ç”¨æ›´å°çš„æ¨¡å‹

### Q4: æ‰¹å¤„ç†å¦‚ä½•å®ç°ï¼Ÿ
A: Python ç»‘å®šæœ¬èº«ä¸æ”¯æŒæ‰¹å¤„ç†ï¼Œä½†å¯ä»¥åœ¨ Python å±‚å®ç°ï¼š
```python
def batch_recognize(audio_list, recognizer):
    results = []
    for audio in audio_list:
        stream = recognizer.create_stream()
        stream.accept_waveform(16000, audio)
        recognizer.decode_stream(stream)
        results.append(stream.result)
    return results
```

## ç›¸å…³æ–‡ä»¶æ¸…å•

### æ ¸å¿ƒç»‘å®šæ–‡ä»¶
- `csrc/offline-recognizer.cc` - ç¦»çº¿ ASR ç»‘å®š
- `csrc/online-recognizer.cc` - åœ¨çº¿ ASR ç»‘å®š
- `csrc/offline-tts.cc` - TTS ç»‘å®š
- `csrc/speaker-embedding-extractor.cc` - è¯´è¯äººåµŒå…¥
- `csrc/vad.cc` - VAD ç»‘å®š

### é…ç½®ç»‘å®šæ–‡ä»¶
- `csrc/offline-model-config.cc` - ç¦»çº¿æ¨¡å‹é…ç½®
- `csrc/online-model-config.cc` - åœ¨çº¿æ¨¡å‹é…ç½®
- `csrc/offline-tts-model-config.cc` - TTS æ¨¡å‹é…ç½®

### å®ç”¨å·¥å…·
- `csrc/alsa.cc` - Linux ALSA éŸ³é¢‘æ¥å£
- `csrc/display.cc` - å®æ—¶æ˜¾ç¤º
- `csrc/circular-buffer.cc` - å¾ªç¯ç¼“å†²åŒº

### ç¤ºä¾‹ä»£ç 
è§ `python-api-examples/` ç›®å½•

## å˜æ›´è®°å½•

### 2025-12-10 07:48:28
- ğŸ“ åˆ›å»º Python ç»‘å®šæ–‡æ¡£
- ğŸ”— æ·»åŠ  API ä½¿ç”¨ç¤ºä¾‹
- ğŸ’¡ è¡¥å……å¸¸è§é—®é¢˜è§£ç­”
- ğŸ“Š åˆ—å‡ºæ‰€æœ‰ç»‘å®šæ¥å£

---

*ç›¸å…³æ–‡ä»¶ï¼š[Python ç¤ºä¾‹](../../python-api-examples/CLAUDE.md) | [C++ æ ¸å¿ƒ](../csrc/CLAUDE.md) | [C API](../c-api/CLAUDE.md)*